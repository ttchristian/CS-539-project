# -*- coding: utf-8 -*-
"""FruitDiesease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RMFByk-m1x7QLWlM4GyxueDblvbXU8EI
"""

!pip install kaggle
from google.colab import files
uploaded = files.upload()

!kaggle datasets list -s fruit

!mkdir -p ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten --unzip -p /content/data

import os
print("Contents of /content/data:")
for root, dirs, files in os.walk('/content/data'):
    level = root.replace('/content/data', '').count(os.sep)
    indent = ' ' * 2 * level
    print(f'{indent}{os.path.basename(root)}/')
    subindent = ' ' * 2 * (level + 1)
    for file in files[:5]:  
        print(f'{subindent}{file}')
    if len(files) > 5:
        print(f'{subindent}... and {len(files)-5} more files')


print(" Dataset downloaded!")
!ls /content/data | head -10

# Check GPU
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\n Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# CELL 2: Import Libraries


import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision import transforms, models
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import time
from collections import defaultdict
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# CELL 3: Configuration


class Config:

    DATA_DIR = "/content/data"


    BATCH_SIZE = 16
    NUM_EPOCHS = 15
    LEARNING_RATE = 0.001
    IMG_SIZE = 224


    TRAIN_RATIO = 0.7
    VAL_RATIO = 0.15
    TEST_RATIO = 0.15


    DROPOUT_RATE = 0.3

config = Config()
print(f" Configuration set: {config.NUM_EPOCHS} epochs, batch size {config.BATCH_SIZE}")

# CELL 4: Dataset Class


class FreshGuardDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.samples = []
        self.classes = []
        self.class_to_idx = {}

        self._load_samples()

    def _load_samples(self):
        """Load all image paths and create class mappings"""
        print(f"ðŸ“ Loading dataset from: {self.data_dir}")


        class_dirs = []
        for item in os.listdir(self.data_dir):
            item_path = os.path.join(self.data_dir, item)
            if os.path.isdir(item_path):
                class_dirs.append(item)

        class_dirs = sorted(class_dirs)


        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_dirs)}
        self.classes = class_dirs

        print(f"Found {len(self.classes)} classes:")


        total_images = 0
        for class_name in class_dirs:
            class_dir = os.path.join(self.data_dir, class_name)
            class_idx = self.class_to_idx[class_name]


            image_files = []
            for file in os.listdir(class_dir):
                if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                    image_files.append(file)


            for img_file in image_files:
                img_path = os.path.join(class_dir, img_file)
                self.samples.append((img_path, class_idx))

            print(f"   {class_name}: {len(image_files)} images")
            total_images += len(image_files)

        print(f" Total samples: {total_images:,}")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]

        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, label
        except Exception as e:
            print(f" Error loading {img_path}: {e}")
            blank_image = Image.new('RGB', (224, 224), color='white')
            if self.transform:
                blank_image = self.transform(blank_image)
            return blank_image, label

# CELL 5: Data Transforms and Splits


def get_transforms():
    """Get training and validation transforms"""
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    return train_transform, val_transform

def create_data_splits(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """Split dataset into train, validation, and test sets"""
    print("\n Creating data splits...")

    indices = list(range(len(dataset)))
    labels = [dataset.samples[i][1] for i in indices]


    train_val_indices, test_indices = train_test_split(
        indices, test_size=test_ratio, random_state=42, stratify=labels
    )


    train_val_labels = [labels[i] for i in train_val_indices]
    val_size = val_ratio / (train_ratio + val_ratio)

    train_indices, val_indices = train_test_split(
        train_val_indices, test_size=val_size, random_state=42, stratify=train_val_labels
    )

    print(f"  Dataset Split Summary:")
    print(f"  Total samples: {len(indices):,}")
    print(f"  Train: {len(train_indices):,} ({len(train_indices)/len(indices):.1%})")
    print(f"  Validation: {len(val_indices):,} ({len(val_indices)/len(indices):.1%})")
    print(f"  Test: {len(test_indices):,} ({len(test_indices)/len(indices):.1%})")

    return train_indices, val_indices, test_indices

# CELL 6: Model Definition


class FreshGuardEfficientNet(nn.Module):
    def __init__(self, num_classes, dropout_rate=0.3):
        super(FreshGuardEfficientNet, self).__init__()


        self.backbone = models.efficientnet_b0(weights='DEFAULT')


        num_features = self.backbone.classifier[1].in_features


        self.backbone.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.7),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        return self.backbone(x)

# CELL 7: Training Function


def train_model(model, train_loader, val_loader, device, num_epochs=12):
    """Training function optimized for Colab"""

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)

    history = defaultdict(list)
    best_val_acc = 0.0

    print(f"\n Starting training for {num_epochs} epochs on {device}...")
    print("=" * 70)

    for epoch in range(num_epochs):
        start_time = time.time()


        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        train_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]")
        for batch_idx, (data, target) in enumerate(train_bar):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            train_total += target.size(0)
            train_correct += (predicted == target).sum().item()


            train_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*train_correct/train_total:.2f}%'
            })


        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            val_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]")
            for data, target in val_bar:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)

                val_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                val_total += target.size(0)
                val_correct += (predicted == target).sum().item()

                val_bar.set_postfix({'Acc': f'{100.*val_correct/val_total:.2f}%'})


        train_loss = train_loss / len(train_loader)
        train_acc = train_correct / train_total
        val_loss = val_loss / len(val_loader)
        val_acc = val_correct / val_total


        scheduler.step(val_loss)


        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), '/content/best_freshguard_model.pth')
            print(f"New best model saved! Val Acc: {val_acc:.4f}")


        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        # Print epoch results
        epoch_time = time.time() - start_time
        print(f"Epoch {epoch+1:2d}/{num_epochs} | "
              f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
              f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | "
              f"Time: {epoch_time:.1f}s")

    print("=" * 70)
    print(f" Training completed! Best validation accuracy: {best_val_acc:.4f}")

    return history, best_val_acc

# CELL 8: Load Data and Create Model



print(" Checking data structure...")
data_root = "/content/data"


dataset_folders = []
for item in os.listdir(data_root):
    item_path = os.path.join(data_root, item)
    if os.path.isdir(item_path):

        subfolders = [f for f in os.listdir(item_path) if os.path.isdir(os.path.join(item_path, f))]
        if len(subfolders) > 1:
            dataset_folders.append(item_path)
            print(f"Found dataset folder: {item_path}")


if dataset_folders:
    config.DATA_DIR = dataset_folders[0]
    print(f" Using dataset path: {config.DATA_DIR}")
else:

    print(f"Using original path: {config.DATA_DIR}")


print("\n Loading dataset...")
train_transform, val_transform = get_transforms()


full_dataset = FreshGuardDataset(config.DATA_DIR, transform=None)
num_classes = len(full_dataset.classes)
class_names = full_dataset.classes

print(f"\nDataset Info:")
print(f"  Classes: {num_classes}")
print(f"  Class names: {class_names}")
print(f"  Total images: {len(full_dataset):,}")


if len(full_dataset) == 0:
    print(" No images found! Let's check the folder structure...")


    for root, dirs, files in os.walk(config.DATA_DIR):
        if dirs:
            print(f"Folder: {root}")
            print(f"  Subfolders: {dirs[:10]}")
        if files:
            image_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            if image_files:
                print(f"  Images: {len(image_files)}")
                break

    raise ValueError("No images found in dataset!")


train_indices, val_indices, test_indices = create_data_splits(full_dataset)


train_dataset = FreshGuardDataset(config.DATA_DIR, transform=train_transform)
val_dataset = FreshGuardDataset(config.DATA_DIR, transform=val_transform)
test_dataset = FreshGuardDataset(config.DATA_DIR, transform=val_transform)


train_subset = Subset(train_dataset, train_indices)
val_subset = Subset(val_dataset, val_indices)
test_subset = Subset(test_dataset, test_indices)


train_loader = DataLoader(train_subset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)
val_loader = DataLoader(val_subset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)
test_loader = DataLoader(test_subset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)

print(f"\n Data Loaders:")
print(f"  Train batches: {len(train_loader)}")
print(f"  Val batches: {len(val_loader)}")
print(f"  Test batches: {len(test_loader)}")


print(f"\n Creating EfficientNet-B0 model...")
model = FreshGuardEfficientNet(num_classes=num_classes, dropout_rate=config.DROPOUT_RATE).to(device)

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"  Total parameters: {total_params:,}")
print(f"  Trainable parameters: {trainable_params:,}")
print(f"  Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB")

print(f"\n Dataset loaded successfully!")
print(f"Ready to train on {num_classes} classes with {len(full_dataset):,} total images!")

# CELL 9: Train the Model

print(f"\n Starting training on {device}...")
history, best_val_acc = train_model(model, train_loader, val_loader, device, config.NUM_EPOCHS)

# CELL 10: Evaluate and Visualize



print(f"\n Loading best model for final evaluation...")
model.load_state_dict(torch.load('/content/best_freshguard_model.pth'))
model.eval()


print(" Final test evaluation...")
all_predictions = []
all_targets = []

with torch.no_grad():
    for data, target in tqdm(test_loader, desc="Testing"):
        data, target = data.to(device), target.to(device)
        output = model(data)
        _, predicted = torch.max(output, 1)

        all_predictions.extend(predicted.cpu().numpy())
        all_targets.extend(target.cpu().numpy())


test_accuracy = accuracy_score(all_targets, all_predictions)


print(f"\n Final Results:")
print(f"  Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)")
print(f"  Final Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")


plt.figure(figsize=(18, 6))


plt.subplot(1, 3, 1)
plt.plot(history['train_loss'], label='Train Loss', linewidth=2, color='blue')
plt.plot(history['val_loss'], label='Val Loss', linewidth=2, color='red')
plt.title('Model Loss', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
plt.plot(history['train_acc'], label='Train Acc', linewidth=2, color='blue')
plt.plot(history['val_acc'], label='Val Acc', linewidth=2, color='red')
plt.title('Model Accuracy', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)


plt.subplot(1, 3, 3)
results = [best_val_acc, test_accuracy]
labels = ['Best Validation', 'Final Test']
colors = ['skyblue', 'lightgreen']

bars = plt.bar(labels, results, color=colors, alpha=0.8)
plt.title('Final Results Comparison', fontsize=14, fontweight='bold')
plt.ylabel('Accuracy')
plt.ylim(0, 1)


for bar, result in zip(bars, results):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{result:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()


print(f"\n Detailed Classification Report:")
report = classification_report(all_targets, all_predictions, target_names=class_names)
print(report)

# CELL 11: Download Trained Model



from google.colab import files

print(" Downloading trained model to your computer...")
files.download('/content/best_freshguard_model.pth')


summary = {
    'model_name': 'EfficientNet-B0',
    'num_classes': num_classes,
    'total_params': total_params,
    'best_val_accuracy': best_val_acc,
    'test_accuracy': test_accuracy,
    'training_epochs': len(history['train_loss']),
    'device_used': str(device)
}

print(f"\n FreshGuard AI Training Complete!")
print(f" Summary:")
for key, value in summary.items():
    print(f"  {key}: {value}")

print(f"\n Your model is ready to use!")
print(f" Model file downloaded: best_freshguard_model.pth")
print(f" You can now use this model in your local projects!")
