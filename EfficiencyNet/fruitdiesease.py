# -*- coding: utf-8 -*-
"""FruitDiesease.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jIa7C9AqYgDsXiVJlXys-2c9bnUhBzGp
"""

!pip install kaggle
from google.colab import files
uploaded = files.upload()

!kaggle datasets list -s fruit

!mkdir -p ~/.kaggle

!cp kaggle.json ~/.kaggle/

!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d muhammad0subhan/fruit-and-vegetable-disease-healthy-vs-rotten --unzip -p /content/data

import os
print("Contents of /content/data:")
for root, dirs, files in os.walk('/content/data'):
    level = root.replace('/content/data', '').count(os.sep)
    indent = ' ' * 2 * level
    print(f'{indent}{os.path.basename(root)}/')
    subindent = ' ' * 2 * (level + 1)
    for file in files[:5]:  # Show first 5 files
        print(f'{subindent}{file}')
    if len(files) > 5:
        print(f'{subindent}... and {len(files)-5} more files')

# Verify download
print(" Dataset downloaded!")
!ls /content/data | head -10

# Check GPU
import torch
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\n Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# =====================================
# CELL 2: Import Libraries
# =====================================

import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision import transforms, models
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import time
from collections import defaultdict
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

# =====================================
# CELL 3: Configuration
# =====================================

class Config:
    # Paths
    DATA_DIR = "/content/data"

    # Training parameters (optimized for Colab)
    BATCH_SIZE = 16  # Higher batch size for GPU
    NUM_EPOCHS = 15  # Reasonable for Colab time limits
    LEARNING_RATE = 0.001
    IMG_SIZE = 224

    # Data split ratios
    TRAIN_RATIO = 0.7
    VAL_RATIO = 0.15
    TEST_RATIO = 0.15

    # Model parameters
    DROPOUT_RATE = 0.3

config = Config()
print(f"‚öôÔ∏è Configuration set: {config.NUM_EPOCHS} epochs, batch size {config.BATCH_SIZE}")

# =====================================
# CELL 4: Dataset Class
# =====================================

class FreshGuardDataset(Dataset):
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.samples = []
        self.classes = []
        self.class_to_idx = {}

        self._load_samples()

    def _load_samples(self):
        """Load all image paths and create class mappings"""
        print(f"üìÅ Loading dataset from: {self.data_dir}")

        # Get all class directories
        class_dirs = []
        for item in os.listdir(self.data_dir):
            item_path = os.path.join(self.data_dir, item)
            if os.path.isdir(item_path):
                class_dirs.append(item)

        class_dirs = sorted(class_dirs)

        # Create class mappings
        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(class_dirs)}
        self.classes = class_dirs

        print(f"Found {len(self.classes)} classes:")

        # Load all samples
        total_images = 0
        for class_name in class_dirs:
            class_dir = os.path.join(self.data_dir, class_name)
            class_idx = self.class_to_idx[class_name]

            # Get all image files
            image_files = []
            for file in os.listdir(class_dir):
                if file.lower().endswith(('.jpg', '.jpeg', '.png')):
                    image_files.append(file)

            # Add to samples
            for img_file in image_files:
                img_path = os.path.join(class_dir, img_file)
                self.samples.append((img_path, class_idx))

            print(f"  üì∏ {class_name}: {len(image_files)} images")
            total_images += len(image_files)

        print(f"üìä Total samples: {total_images:,}")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, label = self.samples[idx]

        try:
            image = Image.open(img_path).convert('RGB')
            if self.transform:
                image = self.transform(image)
            return image, label
        except Exception as e:
            print(f" Error loading {img_path}: {e}")
            blank_image = Image.new('RGB', (224, 224), color='white')
            if self.transform:
                blank_image = self.transform(blank_image)
            return blank_image, label

# =====================================
# CELL 5: Data Transforms and Splits
# =====================================

def get_transforms():
    """Get training and validation transforms"""
    train_transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.RandomCrop(224),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    val_transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    return train_transform, val_transform

def create_data_splits(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """Split dataset into train, validation, and test sets"""
    print("\nüîÑ Creating data splits...")

    indices = list(range(len(dataset)))
    labels = [dataset.samples[i][1] for i in indices]

    # First split: separate test set
    train_val_indices, test_indices = train_test_split(
        indices, test_size=test_ratio, random_state=42, stratify=labels
    )

    # Second split: separate train and validation
    train_val_labels = [labels[i] for i in train_val_indices]
    val_size = val_ratio / (train_ratio + val_ratio)

    train_indices, val_indices = train_test_split(
        train_val_indices, test_size=val_size, random_state=42, stratify=train_val_labels
    )

    print(f"üìä Dataset Split Summary:")
    print(f"  Total samples: {len(indices):,}")
    print(f"  Train: {len(train_indices):,} ({len(train_indices)/len(indices):.1%})")
    print(f"  Validation: {len(val_indices):,} ({len(val_indices)/len(indices):.1%})")
    print(f"  Test: {len(test_indices):,} ({len(test_indices)/len(indices):.1%})")

    return train_indices, val_indices, test_indices

# =====================================
# CELL 6: Model Definition
# =====================================

class FreshGuardEfficientNet(nn.Module):
    def __init__(self, num_classes, dropout_rate=0.3):
        super(FreshGuardEfficientNet, self).__init__()

        # Load pre-trained EfficientNet-B0
        self.backbone = models.efficientnet_b0(weights='DEFAULT')

        # Get number of features
        num_features = self.backbone.classifier[1].in_features

        # Replace classifier
        self.backbone.classifier = nn.Sequential(
            nn.Dropout(dropout_rate),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.7),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.5),
            nn.Linear(256, num_classes)
        )

    def forward(self, x):
        return self.backbone(x)

# =====================================
# CELL 7: Training Function
# =====================================

def train_model(model, train_loader, val_loader, device, num_epochs=12):
    """Training function optimized for Colab"""

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5)

    history = defaultdict(list)
    best_val_acc = 0.0

    print(f"\n Starting training for {num_epochs} epochs on {device}...")
    print("=" * 70)

    for epoch in range(num_epochs):
        start_time = time.time()

        # Training phase
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        train_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Train]")
        for batch_idx, (data, target) in enumerate(train_bar):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            train_total += target.size(0)
            train_correct += (predicted == target).sum().item()

            # Update progress bar
            train_bar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{100.*train_correct/train_total:.2f}%'
            })

        # Validation phase
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0

        with torch.no_grad():
            val_bar = tqdm(val_loader, desc=f"Epoch {epoch+1}/{num_epochs} [Val]")
            for data, target in val_bar:
                data, target = data.to(device), target.to(device)
                output = model(data)
                loss = criterion(output, target)

                val_loss += loss.item()
                _, predicted = torch.max(output.data, 1)
                val_total += target.size(0)
                val_correct += (predicted == target).sum().item()

                val_bar.set_postfix({'Acc': f'{100.*val_correct/val_total:.2f}%'})

        # Calculate metrics
        train_loss = train_loss / len(train_loader)
        train_acc = train_correct / train_total
        val_loss = val_loss / len(val_loader)
        val_acc = val_correct / val_total

        # Learning rate scheduling
        scheduler.step(val_loss)

        # Save best model
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save(model.state_dict(), '/content/best_freshguard_model.pth')
            print(f"üíæ New best model saved! Val Acc: {val_acc:.4f}")

        # Store history
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)

        # Print epoch results
        epoch_time = time.time() - start_time
        print(f"Epoch {epoch+1:2d}/{num_epochs} | "
              f"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | "
              f"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | "
              f"Time: {epoch_time:.1f}s")

    print("=" * 70)
    print(f" Training completed! Best validation accuracy: {best_val_acc:.4f}")

    return history, best_val_acc

# =====================================
# CELL 8: Load Data and Create Model (FIXED VERSION)
# =====================================

# First, let's check the actual data structure
print("üîç Checking data structure...")
data_root = "/content/data"

# Find the actual dataset folder
dataset_folders = []
for item in os.listdir(data_root):
    item_path = os.path.join(data_root, item)
    if os.path.isdir(item_path):
        # Check if this folder contains class folders
        subfolders = [f for f in os.listdir(item_path) if os.path.isdir(os.path.join(item_path, f))]
        if len(subfolders) > 1:  # Likely contains class folders
            dataset_folders.append(item_path)
            print(f"Found dataset folder: {item_path}")

# Use the first dataset folder found, or fall back to original
if dataset_folders:
    config.DATA_DIR = dataset_folders[0]
    print(f"‚úÖ Using dataset path: {config.DATA_DIR}")
else:
    # If no nested folder, use the original path
    print(f"Using original path: {config.DATA_DIR}")

# Load dataset
print("\nüìÅ Loading dataset...")
train_transform, val_transform = get_transforms()

# Load full dataset to get class info
full_dataset = FreshGuardDataset(config.DATA_DIR, transform=None)
num_classes = len(full_dataset.classes)
class_names = full_dataset.classes

print(f"\nüè∑Ô∏è Dataset Info:")
print(f"  Classes: {num_classes}")
print(f"  Class names: {class_names}")
print(f"  Total images: {len(full_dataset):,}")

# Only proceed if we have data
if len(full_dataset) == 0:
    print(" No images found! Let's check the folder structure...")

    # Debug: List all folders in data directory
    for root, dirs, files in os.walk(config.DATA_DIR):
        if dirs:
            print(f"Folder: {root}")
            print(f"  Subfolders: {dirs[:10]}")  # Show first 10
        if files:
            image_files = [f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))]
            if image_files:
                print(f"  Images: {len(image_files)}")
                break

    raise ValueError("No images found in dataset!")

# Create data splits
train_indices, val_indices, test_indices = create_data_splits(full_dataset)

# Create datasets with transforms
train_dataset = FreshGuardDataset(config.DATA_DIR, transform=train_transform)
val_dataset = FreshGuardDataset(config.DATA_DIR, transform=val_transform)
test_dataset = FreshGuardDataset(config.DATA_DIR, transform=val_transform)

# Create data subsets
train_subset = Subset(train_dataset, train_indices)
val_subset = Subset(val_dataset, val_indices)
test_subset = Subset(test_dataset, test_indices)

# Create data loaders
train_loader = DataLoader(train_subset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2)
val_loader = DataLoader(val_subset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)
test_loader = DataLoader(test_subset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)

print(f"\nüì¶ Data Loaders:")
print(f"  Train batches: {len(train_loader)}")
print(f"  Val batches: {len(val_loader)}")
print(f"  Test batches: {len(test_loader)}")

# Create model
print(f"\nü§ñ Creating EfficientNet-B0 model...")
model = FreshGuardEfficientNet(num_classes=num_classes, dropout_rate=config.DROPOUT_RATE).to(device)

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(f"  Total parameters: {total_params:,}")
print(f"  Trainable parameters: {trainable_params:,}")
print(f"  Model size: ~{total_params * 4 / 1024 / 1024:.1f} MB")

print(f"\nüéâ Dataset loaded successfully!")
print(f"üìä Ready to train on {num_classes} classes with {len(full_dataset):,} total images!")

# =====================================
# CELL 9: Train the Model
# =====================================

# Start training
print(f"\n Starting training on {device}...")
history, best_val_acc = train_model(model, train_loader, val_loader, device, config.NUM_EPOCHS)

# =====================================
# CELL 10: Evaluate and Visualize
# =====================================

# Load best model for evaluation
print(f"\n Loading best model for final evaluation...")
model.load_state_dict(torch.load('/content/best_freshguard_model.pth'))
model.eval()

# Test evaluation
print(" Final test evaluation...")
all_predictions = []
all_targets = []

with torch.no_grad():
    for data, target in tqdm(test_loader, desc="Testing"):
        data, target = data.to(device), target.to(device)
        output = model(data)
        _, predicted = torch.max(output, 1)

        all_predictions.extend(predicted.cpu().numpy())
        all_targets.extend(target.cpu().numpy())

# Calculate final test accuracy
test_accuracy = accuracy_score(all_targets, all_predictions)

# Print results
print(f"\n Final Results:")
print(f"  Best Validation Accuracy: {best_val_acc:.4f} ({best_val_acc*100:.2f}%)")
print(f"  Final Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)")

# Plot training history
plt.figure(figsize=(18, 6))

# Loss plot
plt.subplot(1, 3, 1)
plt.plot(history['train_loss'], label='Train Loss', linewidth=2, color='blue')
plt.plot(history['val_loss'], label='Val Loss', linewidth=2, color='red')
plt.title('Model Loss', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, alpha=0.3)

# Accuracy plot
plt.subplot(1, 3, 2)
plt.plot(history['train_acc'], label='Train Acc', linewidth=2, color='blue')
plt.plot(history['val_acc'], label='Val Acc', linewidth=2, color='red')
plt.title('Model Accuracy', fontsize=14, fontweight='bold')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True, alpha=0.3)

# Final results comparison
plt.subplot(1, 3, 3)
results = [best_val_acc, test_accuracy]
labels = ['Best Validation', 'Final Test']
colors = ['skyblue', 'lightgreen']

bars = plt.bar(labels, results, color=colors, alpha=0.8)
plt.title('Final Results Comparison', fontsize=14, fontweight='bold')
plt.ylabel('Accuracy')
plt.ylim(0, 1)

# Add value labels on bars
for bar, result in zip(bars, results):
    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
             f'{result:.3f}', ha='center', va='bottom', fontweight='bold')

plt.tight_layout()
plt.show()

# Classification report
print(f"\n Detailed Classification Report:")
report = classification_report(all_targets, all_predictions, target_names=class_names)
print(report)

# =====================================
# CELL 11: Download Trained Model
# =====================================

# Download the trained model
from google.colab import files

print(" Downloading trained model to your computer...")
files.download('/content/best_freshguard_model.pth')

# Create summary
summary = {
    'model_name': 'EfficientNet-B0',
    'num_classes': num_classes,
    'total_params': total_params,
    'best_val_accuracy': best_val_acc,
    'test_accuracy': test_accuracy,
    'training_epochs': len(history['train_loss']),
    'device_used': str(device)
}

print(f"\n FreshGuard AI Training Complete!")
print(f" Summary:")
for key, value in summary.items():
    print(f"  {key}: {value}")

print(f"\n Your model is ready to use!")
print(f" Model file downloaded: best_freshguard_model.pth")
print(f" You can now use this model in your local projects!")